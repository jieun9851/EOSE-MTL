{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from kobert.utils import get_tokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AdamW\n",
    "from seqeval.metrics import f1_score\n",
    "from models.layers.linears import PoolerEndLogits, PoolerStartLogits\n",
    "from losses.focal_loss import FocalLoss\n",
    "from losses.label_smoothing import LabelSmoothingCrossEntropy\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from processors.utils_ner import bert_extract_item as bert_extract_item_pred\n",
    "from metrics.ner_metrics import SpanEntityScore\n",
    "from tools.common import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processors.utils_ner import DataProcessor, get_entities\n",
    "from processors.ner_span import InputExample, InputFeature\n",
    "from torch.utils.data import TensorDataset\n",
    "from processors.ner_span import convert_examples_to_features, CnerProcessor\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import logging\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd\n"
     ]
    }
   ],
   "source": [
    "  label_list =['O','PDT','MOV','TRV']\n",
    "  id2label = {i: label for i, label in enumerate(label_list)}\n",
    "  metric = SpanEntityScore(id2label)\n",
    "\n",
    "  start_answer = tf.math.argmax(start_pred,-1) # b * seq\n",
    "  end_answer = tf.math.argmax(end_pred,-1)\n",
    "\n",
    "  active_loss = attention_mask==1\n",
    "\n",
    "  for i in range(active_loss.shape[0]):\n",
    "    active_start_pred = tf.boolean_mask(start_answer[i], active_loss[i])\n",
    "    active_end_pred = tf.boolean_mask(end_answer[i], active_loss[i])\n",
    "    R = bert_extract_item(active_start_pred, active_end_pred)\n",
    "    T = bert_extract_item(start_label[i], end_label[i])\n",
    "    metric.update(true_subject=T, pred_subject=R)\n",
    "  eval_info, entity_info = metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "num = 4\n",
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model() # KoBERT 모델 불러오기\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSpanForNer(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=num, dr_rate=0.3, params=None):\n",
    "        super(BertSpanForNer, self).__init__()\n",
    "        self.soft_label = True\n",
    "        self.num_labels = num_classes\n",
    "        # loss_type = ['lsr', 'focal', 'ce']\n",
    "        self.loss_type = 'ce'\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(dr_rate)\n",
    "        self.start_fc = PoolerStartLogits(hidden_size, self.num_labels)\n",
    "        if self.soft_label:\n",
    "            self.end_fc = PoolerEndLogits(hidden_size + self.num_labels, self.num_labels)\n",
    "        else:\n",
    "            self.end_fc = PoolerEndLogits(hidden_size + 1, self.num_labels)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "        \n",
    "    def forward(self, input_ids, valid_length, token_type_ids=None, start_positions=None, end_positions=None):\n",
    "        attention_mask = self.gen_attention_mask(input_ids, valid_length)\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask.float().to(input_ids.device))\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        start_logits = self.start_fc(sequence_output)\n",
    "        \n",
    "        if start_positions is not None and self.training:\n",
    "            if self.soft_label:\n",
    "                batch_size = input_ids.size(0)\n",
    "                seq_len = input_ids.size(1)\n",
    "                label_logits = torch.FloatTensor(batch_size, seq_len, self.num_labels)\n",
    "                label_logits.zero_()\n",
    "                label_logits = label_logits.to(input_ids.device)\n",
    "                label_logits.scatter_(2, start_positions.unsqueeze(2), 1)\n",
    "            else:\n",
    "                label_logits = start_positions.unsqueeze(2).float()\n",
    "        else:\n",
    "            label_logits = F.softmax(start_logits, -1)\n",
    "            if not self.soft_label:\n",
    "                label_logits = torch.argmax(label_logits, -1).unsqueeze(2).float()\n",
    "                \n",
    "        end_logits = self.end_fc(sequence_output, label_logits)\n",
    "        outputs = (start_logits, end_logits,) + outputs[2:]\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            assert self.loss_type in ['lsr', 'focal', 'ce']\n",
    "            if self.loss_type =='lsr':\n",
    "                loss_fct = LabelSmoothingCrossEntropy()\n",
    "            elif self.loss_type == 'focal':\n",
    "                loss_fct = FocalLoss()\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "            start_logits = start_logits.view(-1, self.num_labels)\n",
    "            end_logits = end_logits.view(-1, self.num_labels)\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_start_logits = start_logits[active_loss]\n",
    "            active_end_logits = end_logits[active_loss]\n",
    "\n",
    "            active_start_labels = start_positions.view(-1)[active_loss]\n",
    "            active_end_labels = end_positions.view(-1)[active_loss]\n",
    "\n",
    "            start_loss = loss_fct(active_start_logits, active_start_labels)\n",
    "            end_loss = loss_fct(active_end_logits, active_end_labels)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "            outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'O': 0,\n",
    " 'UNK':1,\n",
    " 'PDT-B': 2,\n",
    " 'PDT-I': 3,\n",
    " 'MOV-B': 4,\n",
    " 'MOV-I': 5,\n",
    " 'TRV-B': 6,\n",
    " 'TRV-I': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18390, 18390, 4599, 4599)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list_csv = pd.read_csv('train_data.tsv',delimiter='\\t')\n",
    "test_list_csv = pd.read_csv('test_data.tsv',delimiter='\\t')\n",
    "\n",
    "train_list_csv = train_list_csv.dropna(axis=0)\n",
    "test_list_csv = test_list_csv.dropna(axis=0)\n",
    "\n",
    "train_list_csv = train_list_csv.reset_index(drop=True)\n",
    "test_list_csv = test_list_csv.reset_index(drop=True)\n",
    "\n",
    "for i in range(len(train_list_csv['label'])):\n",
    "    train_list_csv['label'][i] = train_list_csv['label'][i][1:-1].replace('\\'','').replace(' ','').split(\",\")\n",
    "for i in range(len(test_list_csv['label'])):\n",
    "    test_list_csv['label'][i] = test_list_csv['label'][i][1:-1].replace('\\'','').replace(' ','').split(\",\")\n",
    "\n",
    "\n",
    "for i in range(len(train_list_csv['label'])):\n",
    "    text_split = train_list_csv['text'][i].split()\n",
    "    ex_label = []\n",
    "    for j in range(len(train_list_csv['label'][i])):\n",
    "        ex_label.append(label_dict[train_list_csv['label'][i][j]])\n",
    "    train_list_csv['label'][i]=ex_label\n",
    "\n",
    "for i in range(len(test_list_csv['label'])):\n",
    "    text_split = test_list_csv['text'][i].split()\n",
    "    ex_label = []\n",
    "    for j in range(len(test_list_csv['label'][i])):\n",
    "        ex_label.append(label_dict[test_list_csv['label'][i][j]])\n",
    "    test_list_csv['label'][i]=ex_label\n",
    "\n",
    "tr_tag = train_list_csv['label']\n",
    "tr_sent = train_list_csv['text']\n",
    "ts_tag = test_list_csv['label']\n",
    "ts_sent = test_list_csv['text']\n",
    "\n",
    "len(tr_tag), len(tr_sent), len(ts_tag), len(ts_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18029, 18029, 4514, 4514)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(tr_tag)):\n",
    "    if len(tr_tag[i]) > max_len or len(tr_sent[i]) > max_len: # 문장의 길이가 512가 넘는 문장 제거\n",
    "        del tr_sent[i]\n",
    "        del tr_tag[i]\n",
    "\n",
    "for i in range(len(ts_tag)):\n",
    "    if len(ts_tag[i]) > max_len or len(ts_sent[i]) > max_len: # 문장의 길이가 512가 넘는 문장 제거\n",
    "        del ts_sent[i]\n",
    "        del ts_tag[i]\n",
    "\n",
    "tr_sent = tr_sent.reset_index(drop=True)\n",
    "tr_tag = tr_tag.reset_index(drop=True)\n",
    "\n",
    "ts_sent = ts_sent.reset_index(drop=True)\n",
    "ts_tag = ts_tag.reset_index(drop=True)\n",
    "\n",
    "len(tr_tag), len(tr_sent), len(ts_tag), len(ts_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_token(sent, label):\n",
    "    label_list = []\n",
    "    sent_split = sent.split()\n",
    "    for i in range(len(sent_split)):\n",
    "        sent_tok = tok(sent_split[i])\n",
    "        if label[i] !=0 and label[i]%2==0:\n",
    "            label_list.append(label[i])\n",
    "            for j in sent_tok[1:]:\n",
    "                label_list.append(label[i]+1)\n",
    "        else: \n",
    "            for j in sent_tok:\n",
    "                label_list.append(label[i])\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18029, 18029, 4514, 4514)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_label = []\n",
    "ts_label = []\n",
    "\n",
    "for i,j in zip(tr_sent, tr_tag):\n",
    "    tr_label.append(make_label_token(i,j))\n",
    "\n",
    "for i,j in zip(ts_sent, ts_tag):\n",
    "    ts_label.append(make_label_token(i,j))\n",
    "\n",
    "len(tr_label), len(tr_sent), len(ts_label), len(ts_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_end_pos(label):\n",
    "    new = []\n",
    "    ex = []\n",
    "    result = []\n",
    "    for i in range(len(label)):\n",
    "        if label[i]%2==0 and label[i]!=0:\n",
    "            if len(new) != 0:\n",
    "                ex.append(new)\n",
    "            new = []\n",
    "            new.extend([int(label[i]/2),i])\n",
    "        elif label[i]%2==1:\n",
    "            new.append(i) \n",
    "            \n",
    "    ex.append(new) \n",
    "    for i in ex:\n",
    "        result.append([i[1],i[-1], i[0]])\n",
    "    return result\n",
    "\n",
    "def start_end_make(label):\n",
    "\n",
    "    start_label = [0]*len(label)\n",
    "    end_label = [0]*len(label)\n",
    "\n",
    "    if 2 in label or 4 in label or 6 in label:\n",
    "        result = start_end_pos (label)\n",
    "        for start, end, tag in result:\n",
    "            start_label[start] =  tag\n",
    "            end_label[end] = tag\n",
    "\n",
    "    \n",
    "    start_label.insert(0,0)\n",
    "    end_label.insert(0,0)\n",
    "    return [start_label, end_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tr_label)):\n",
    "    tr_label[i] = start_end_make(tr_label[i])\n",
    "\n",
    "for i in range(len(ts_label)):\n",
    "    ts_label[i] = start_end_make(ts_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18029, 18029, 4514, 4514)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(tr_label)):\n",
    "    padding_length = max_len - len(tr_label[i][0])\n",
    "    tr_label[i][0] = np.array(tr_label[i][0] + ([0] * padding_length))\n",
    "    tr_label[i][0] =tr_label[i][0].astype(np.int64) \n",
    "    tr_label[i][1] = np.array(tr_label[i][1] + ([0] * padding_length))\n",
    "    tr_label[i][1] =tr_label[i][1].astype(np.int64) \n",
    "\n",
    "for i in range(len(ts_label)):\n",
    "    padding_length = max_len - len(ts_label[i][0])\n",
    "    ts_label[i][0] = np.array(ts_label[i][0] + ([0] * padding_length))\n",
    "    ts_label[i][0] =ts_label[i][0].astype(np.int64) \n",
    "    ts_label[i][1] = np.array(ts_label[i][1] + ([0] * padding_length))\n",
    "    ts_label[i][1] =ts_label[i][1].astype(np.int64) \n",
    "\n",
    "len(tr_label), len(tr_sent), len(ts_label), len(ts_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, sent, tag, bert_tokenizer, max_len,\n",
    "                pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i]) for i in sent] #문장\n",
    "        self.labels = tag\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i][0], )+ (self.labels[i][1], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame([tr_sent.tolist(),tr_label]).T\n",
    "test_data =  pd.DataFrame([ts_sent.tolist(),ts_label]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(tr_sent,tr_label, tok, max_len, True, False)\n",
    "data_test = BERTDataset(ts_sent,ts_label, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSpanForNer(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (start_fc): PoolerStartLogits(\n",
       "    (dense): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       "  (end_fc): PoolerEndLogits(\n",
       "    (dense_0): Linear(in_features=772, out_features=772, bias=True)\n",
       "    (activation): Tanh()\n",
       "    (LayerNorm): LayerNorm((772,), eps=1e-05, elementwise_affine=True)\n",
       "    (dense_1): Linear(in_features=772, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = BertSpanForNer(bertmodel)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_loader)\n",
    "weight_decay = 0.01\n",
    "warmup_proportion = 0.1\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "bert_parameters = model.bert.named_parameters()\n",
    "start_parameters = model.start_fc.named_parameters()\n",
    "end_parameters = model.end_fc.named_parameters()\n",
    "optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in bert_parameters if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": weight_decay, 'lr': learning_rate},\n",
    "        {\"params\": [p for n, p in bert_parameters if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0\n",
    "            , 'lr': learning_rate},\n",
    "\n",
    "        {\"params\": [p for n, p in start_parameters if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": weight_decay, 'lr': 0.001},\n",
    "        {\"params\": [p for n, p in start_parameters if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0\n",
    "            , 'lr': 0.001},\n",
    "\n",
    "        {\"params\": [p for n, p in end_parameters if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": weight_decay, 'lr': 0.001},\n",
    "        {\"params\": [p for n, p in end_parameters if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0\n",
    "            , 'lr': 0.001},\n",
    "]\n",
    "\n",
    "warmup_steps = int(t_total * warmup_proportion)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list =['O','PDT','MOV','TRV']\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "metric = SpanEntityScore(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_extract_item_true(start_logits, end_logits):\n",
    "    S = []    \n",
    "    for i, s_l in enumerate(start_logits):\n",
    "        if s_l == 0:\n",
    "            continue\n",
    "        for j, e_l in enumerate(end_logits[i:]):\n",
    "            if s_l == e_l:\n",
    "                S.append((s_l, i, i + j))\n",
    "                break\n",
    "    return S\n",
    "\n",
    "def score_result(y_start_pred, y_end_pred, y_start_true, y_end_true, y_len, loss):\n",
    "    metric = SpanEntityScore(id2label)\n",
    "        \n",
    "    for i in range(len(y_start_pred)):\n",
    "        start_logits = torch.tensor(np.array([y_start_pred[i][:y_len[i]].tolist()]), dtype=torch.long)\n",
    "        end_logits = torch.tensor(np.array([ y_end_pred[i][:y_len[i]].tolist()]), dtype=torch.long)\n",
    "        start_pos_batch = np.array(y_start_true[i][:y_len[i]].tolist())[1:-1]\n",
    "        end_pos_batch = np.array(y_end_true[i][:y_len[i]].tolist())[1:-1]\n",
    "\n",
    "        R = bert_extract_item_pred(start_logits, end_logits)\n",
    "        T = bert_extract_item_true(start_pos_batch, end_pos_batch)\n",
    "\n",
    "\n",
    "        metric.update(true_subject=T, pred_subject=R)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    eval_info, entity_info = metric.result()\n",
    "\n",
    "    results = {f'{key}': value for key, value in eval_info.items()}\n",
    "    results['loss'] = loss\n",
    "    print(\"***** Eval results *****\")\n",
    "    info = \"-\".join([f' {key}: {value:.4f} ' for key, value in results.items()])\n",
    "    print(info)\n",
    "    print(\"***** Entity results *****\")\n",
    "    for key in sorted(entity_info.keys()):\n",
    "        print(\"******* %s results ********\" % key)\n",
    "        info = \"-\".join([f' {key}: {value:.4f} ' for key, value in entity_info[key].items()])\n",
    "        print(info)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [07:28<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.6254 - recall: 0.1627 - f1: 0.2582 - loss: 0.0000 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.4787 - recall: 0.0518 - f1: 0.0934 \n",
      "******* PDT results ********\n",
      " acc: 0.6467 - recall: 0.2457 - f1: 0.3561 \n",
      "******* TRV results ********\n",
      " acc: 0.6591 - recall: 0.0208 - f1: 0.0404 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:30<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.6337 - recall: 0.4457 - f1: 0.5234 - loss: 0.0004 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.5182 - recall: 0.3950 - f1: 0.4483 \n",
      "******* PDT results ********\n",
      " acc: 0.6984 - recall: 0.5224 - f1: 0.5977 \n",
      "******* TRV results ********\n",
      " acc: 0.4865 - recall: 0.1101 - f1: 0.1796 \n",
      "new f1 score\n",
      "epoch: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [07:20<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.6881 - recall: 0.4022 - f1: 0.5076 - loss: 0.0000 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.5571 - recall: 0.2571 - f1: 0.3518 \n",
      "******* PDT results ********\n",
      " acc: 0.7376 - recall: 0.5253 - f1: 0.6136 \n",
      "******* TRV results ********\n",
      " acc: 0.5955 - recall: 0.1342 - f1: 0.2191 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:29<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.6370 - recall: 0.5125 - f1: 0.5680 - loss: 0.0003 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.6398 - recall: 0.2742 - f1: 0.3839 \n",
      "******* PDT results ********\n",
      " acc: 0.6433 - recall: 0.6821 - f1: 0.6621 \n",
      "******* TRV results ********\n",
      " acc: 0.5000 - recall: 0.1713 - f1: 0.2551 \n",
      "new f1 score\n",
      "epoch: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [07:25<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.7635 - recall: 0.5544 - f1: 0.6424 - loss: 0.0000 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.6645 - recall: 0.4173 - f1: 0.5126 \n",
      "******* PDT results ********\n",
      " acc: 0.8126 - recall: 0.6608 - f1: 0.7289 \n",
      "******* TRV results ********\n",
      " acc: 0.6836 - recall: 0.3582 - f1: 0.4701 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:29<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.6895 - recall: 0.4729 - f1: 0.5610 - loss: 0.0003 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.6369 - recall: 0.3145 - f1: 0.4211 \n",
      "******* PDT results ********\n",
      " acc: 0.7391 - recall: 0.5831 - f1: 0.6519 \n",
      "******* TRV results ********\n",
      " acc: 0.4175 - recall: 0.2630 - f1: 0.3227 \n",
      "epoch: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [07:15<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.8319 - recall: 0.6765 - f1: 0.7462 - loss: 0.0000 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.7587 - recall: 0.5533 - f1: 0.6400 \n",
      "******* PDT results ********\n",
      " acc: 0.8701 - recall: 0.7604 - f1: 0.8115 \n",
      "******* TRV results ********\n",
      " acc: 0.7890 - recall: 0.5664 - f1: 0.6594 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:30<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.6394 - recall: 0.5087 - f1: 0.5666 - loss: 0.0003 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.5601 - recall: 0.3845 - f1: 0.4559 \n",
      "******* PDT results ********\n",
      " acc: 0.7186 - recall: 0.6039 - f1: 0.6563 \n",
      "******* TRV results ********\n",
      " acc: 0.3369 - recall: 0.2875 - f1: 0.3102 \n",
      "epoch: 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1127/1127 [07:28<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.8803 - recall: 0.7573 - f1: 0.8142 - loss: 0.0000 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.8244 - recall: 0.6513 - f1: 0.7277 \n",
      "******* PDT results ********\n",
      " acc: 0.9125 - recall: 0.8227 - f1: 0.8653 \n",
      "******* TRV results ********\n",
      " acc: 0.8399 - recall: 0.7006 - f1: 0.7640 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:30<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.5740 - recall: 0.5729 - f1: 0.5734 - loss: 0.0002 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.5032 - recall: 0.4516 - f1: 0.4760 \n",
      "******* PDT results ********\n",
      " acc: 0.6551 - recall: 0.6594 - f1: 0.6573 \n",
      "******* TRV results ********\n",
      " acc: 0.3148 - recall: 0.3976 - f1: 0.3514 \n",
      "new f1 score\n"
     ]
    }
   ],
   "source": [
    "max = 0.0\n",
    "model.zero_grad()\n",
    "\n",
    "for i in range(epochs):\n",
    "  print(\"epoch: \"+str(i+1)+\"/\"+str(epochs))\n",
    "  model.train()\n",
    "  y_start_pred = []\n",
    "  y_end_pred = []\n",
    "  y_start_true = []\n",
    "  y_end_true = []\n",
    "  y_len = []\n",
    "  batch_step = 0\n",
    "\n",
    "\n",
    "  for input_ids_batch, valid_length_batch, segment_ids_batch, start_pos_batch, end_pos_batch in tqdm(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    loss, start_logits, end_logits = model(input_ids_batch.to(device), valid_length_batch.to(device), segment_ids_batch.to(device), start_pos_batch.to(device), end_pos_batch.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "    total_loss = loss.item()\n",
    "    batch_step +=1\n",
    "    optimizer.step()\n",
    "    #scheduler.step()  # Update learning rate schedul\n",
    "    model.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    y_start_pred.extend(start_logits)\n",
    "    y_end_pred.extend(end_logits)\n",
    "    y_start_true.extend(start_pos_batch)\n",
    "    y_end_true.extend(end_pos_batch)\n",
    "    y_len.extend(valid_length_batch)\n",
    "\n",
    "  loss_train = total_loss / batch_step\n",
    "  \n",
    "  result_train = score_result(y_start_pred, y_end_pred, y_start_true, y_end_true, y_len, loss_train)\n",
    "\n",
    "  \n",
    "torch.save(model,\"kobert_span\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"kobert_span\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:32<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Eval results *****\n",
      " acc: 0.6116 - recall: 0.5428 - f1: 0.5752 - loss: 0.0005 \n",
      "***** Entity results *****\n",
      "******* MOV results ********\n",
      " acc: 0.5033 - recall: 0.4353 - f1: 0.4668 \n",
      "******* PDT results ********\n",
      " acc: 0.6639 - recall: 0.6467 - f1: 0.6552 \n",
      "******* TRV results ********\n",
      " acc: 0.5308 - recall: 0.2110 - f1: 0.3020 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.6116129032258064,\n",
       " 'recall': 0.5427998854852563,\n",
       " 'f1': 0.5751554679205219,\n",
       " 'loss': 0.0004545488121652772}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_start_pred_test = []\n",
    "y_end_pred_test = []\n",
    "y_start_true_test = []\n",
    "y_end_true_test = []\n",
    "y_len_test = []\n",
    "batch_step_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch_test, valid_length_batch_test, segment_ids_batch_test, start_pos_batch_test, end_pos_batch_test in tqdm(test_loader):\n",
    "        loss, start_logits_test, end_logits_test = model(input_ids_batch_test.to(device), valid_length_batch_test.to(device), segment_ids_batch_test.to(device), start_pos_batch_test.to(device), end_pos_batch_test.to(device))\n",
    "\n",
    "        total_loss_test = loss.item()\n",
    "        batch_step_test +=1\n",
    "\n",
    "        y_start_pred_test.extend(start_logits_test)\n",
    "        y_end_pred_test.extend(end_logits_test)\n",
    "        y_start_true_test.extend(start_pos_batch_test)\n",
    "        y_end_true_test.extend(end_pos_batch_test)\n",
    "        y_len_test.extend(valid_length_batch_test)\n",
    "\n",
    "loss_test = total_loss_test / batch_step_test\n",
    "score_result(y_start_pred_test, y_end_pred_test, y_start_true_test, y_end_true_test, y_len_test, loss_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
